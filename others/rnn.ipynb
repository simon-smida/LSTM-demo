{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(input_size, output_size):\n",
    "    return np.zeros((input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = zero_init(self.vocab_size, 1)\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            #through softmax\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "            #calculate dV, dc\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dc\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "            \n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) \n",
    "            \n",
    "        return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        return sum(-np.log(ps[t][targets[t], 0]) for t in range(self.seq_length))\n",
    "    \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                      [dU, dW, dV, db, dc],\n",
    "                                      [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        sample a sequence of integers from the model\n",
    "        h is memory state, seed_ix is seed letter from the first time step\n",
    "        \"\"\"\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        \n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "            \n",
    "        return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "        iter_num = 0\n",
    "        threshold = 0.01\n",
    "        smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "        \n",
    "        while (smooth_loss > threshold):\n",
    "            \n",
    "            if data_reader.just_started():\n",
    "                hprev = np.zeros((self.hidden_size, 1))\n",
    "                \n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            \n",
    "            # Forward pass \n",
    "            xs, hs, ps = self.forward(inputs, hprev)\n",
    "            # Backward pass\n",
    "            dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "            # calculate loss\n",
    "            loss = self.loss(ps, targets)\n",
    "            # update model parameters\n",
    "            self.update_model(dU, dW, dV, db, dc)\n",
    "            \n",
    "            smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "            hprev = hs[self.seq_length-1]\n",
    "            if not iter_num % 500:\n",
    "                sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                \n",
    "            iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "        # Initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            # p = np.exp(y)/np.sum(np.exp(y)) # softmax\n",
    "            p = self.softmax(y)\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = zero_init(self.vocab_size, 1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrmlnAobNtxtboputrXUtruktfot\n",
      "wtotogLLAUwzt0Myvu\n",
      "wo”LovoRârSz atobrwfoktAL”rtAwMstheofoFf isotfâoIwrtsturgw ww AtroftiT-LnwM irofguxc€utoowxoks w rzhcolwwroro,M(h okooRor, LotLTBrpAtoFMr1rttwoTtwttrtst\n",
      "\n",
      "\n",
      "iter :0, loss:98.781104\n",
      "s wthen”u sFpbe s ee ge te retâ r rriIn eureulmd umsLesec aa bre ute re it par.r.as af or e inpasnmekeead antNNprurh. r.Tse s ta herpainoumal oeaocas imeaa whrtMe srnalAormute ntofsk-vaycen talr ein t\n",
      "\n",
      "\n",
      "iter :500, loss:90.639975\n",
      " tor thelpaw  eres re ral, on- ges.y wavem toini,, ymaghes theed ,oniv on. BS ot sais me) sawo ororeuoly al se nsometier sercorinmofnpdemwain (1) preE om she thelins Xex des astothe oSzM a) lredemirge\n",
      "\n",
      "\n",
      "iter :1000, loss:79.999583\n",
      "t ort or feim s teson of ineuthin.,)usred metwork 0fpduches ssmar seserradedissth sit ont orpumat on urangess snd taodemobom mtpumtest rrybhM thesinaude thal waItesy, setale seuste tide iogvalo. aqpep\n",
      "\n",
      "\n",
      "iter :1500, loss:69.547340\n",
      " s os of input uralas ant oft,r sexrse next Xingegoprtogk seti roel deafn\n",
      "sitocen is wor whrun und Is if inng nedwork.whe the., RNNut,, recorven de toprededinnet.\n",
      "tin(e coath Rut at or laz ba. â€ke bi\n",
      "\n",
      "\n",
      "iter :2000, loss:60.725202\n",
      " baas of pacesvaral tem th ral-t or os an.\n",
      "\n",
      "Aredibg onivet aac gerelagn thiy lom selionl to on ceras ertion, Betpr prostien f€rer timy, ir led ir., iry tere co lall the wige X(1) ressitnder matwl tore\n",
      "\n",
      "\n",
      "iter :2500, loss:55.059331\n",
      " comes, thies al makaie lemurys thes nN” Tien ithe pexon, patrwor.ured of fars un paobl frocy XS1) from the para rysworltworks ared theyn Bot sorctiog a(” irinputicima temetiein. (U0n tes are tedan sn\n",
      "\n",
      "\n",
      "iter :3000, loss:48.765395\n",
      "s tan pomes ksion torra loneurnoRt tien incogneqices is fhe selmomben sepet erkoeg froml-Nse ther the com ofte ut inpel, ort prmons the ne\n",
      "wis in wal sathe ost in XSTMe st, the ontousif ont ore ret er\n",
      "\n",
      "\n",
      "iter :3500, loss:43.882634\n",
      " maces the sece nextodikallerge the input.\n",
      "\n",
      "Un premresstiom. Stho sagard tpreti (k ar ta inemomned to ter arsererralkertorn, Atoqice pucmes input asper. Siops fothe pnecisif med seqsorts arg at aed so\n",
      "\n",
      "\n",
      "iter :4000, loss:38.569108\n",
      " uskesith X(1) rest-wo hemory, inpet the ormertingmin tidin. 0moquangnimey and rmoree terve neratit ar er cally the sechidasitg nut angur ta pas tpder the l(0s rsken dath nestithm reedenstoe the duine\n",
      "\n",
      "\n",
      "iter :4500, loss:34.292143\n",
      "e togpund sor inudingso te wroly en feed ined ane taknsito X(2d as uramatiog, math works the soFpent int ros are pumeste pant prserrworks, nuwhikh Xomes entecwingeinput. The nerediggnall teent int out\n",
      "\n",
      "\n",
      "iter :5000, loss:30.305954\n",
      " the rachine furprend relceros in) serald pal) fust, oneutput. This are connatper.\n",
      "They from grare usodigg the rama litpand pros mara puturr on. Thes and serscang at of h-aurnt ofoting the whime whiva\n",
      "\n",
      "\n",
      "iter :5500, loss:27.167692\n",
      " ef inieg input theind preve remess itith. ras, in tounal networks are fumall neutemor the tige sognemve sext dith X(2), in vecisinald dital sourall url basis en el) inptworks the humatg XnN)-whis mac\n",
      "\n",
      "\n",
      "iter :6000, loss:24.343262\n",
      " recegsiL(0) an Meropes from the torell-Me nember sithe rpmeusting a thie wand gs vemuthe ceceaf inge nelrass dora lentoft nerichist) ward neura de veceurof input fron lNNe cext in utwoant congSpeute,\n",
      "\n",
      "\n",
      "iter :6500, loss:23.092356\n",
      " tiey insot orked recestiz ticingurment in furral networks are a gsoredland pard recegning the cosignos the next inal torither the las tion. Feand proby is lNed cabl taw it gaw input an seminel) the c\n",
      "\n",
      "\n",
      "iter :7000, loss:20.238156\n",
      "s are se1 eutis. it, inputputmol. Afursting, hake werstif networks, wo coms which cabl-sonh the harand fras ther rate ingermer bA (lemesify ingodhprered more. The X(0) ciclatie, vata ne.\n",
      "Tint intodton\n",
      "\n",
      "\n",
      "iter :7500, loss:18.156615\n",
      " races the vas as protior mecuste Arecrel itberks ate X(1) fso ves of ale set of allin tiention, base netrorks all put and neurend io sucervestad hercornsstwrrlargech nusurge neural networks. which sa\n",
      "\n",
      "\n",
      "iter :8000, loss:16.239386\n",
      " fagen dMeswarly, an dem mece nembersstain duwral netifr ..\n",
      "RNN,is, neut is the inputs ho LSTM neural networks, lhN al) Fran â€”ian, putves the copnelle s lumserepag siop. SM an LSTh sechiet, wrich so\n",
      "\n",
      "\n",
      "iter :8500, loss:14.743011\n",
      " tice wrrmerrenpustion fontore palvis censogw prous leutrel. Ipeergnibly ricussing. LSTMuind the caw is member the hemve sigilly ther torether thennext from mata the recutren wormlTs sthe semvesien) t\n",
      "\n",
      "\n",
      "iter :9000, loss:13.234633\n",
      "sey fhom th int rather whal nathet prodereprect recl-surks and are cestgn. S1uric) inksith, rom the therin tMer ruve ther the sellestworks ard ale cuselly the ieprecod con ic network, thre h(0) anlast\n",
      "\n",
      "\n",
      "iter :9500, loss:12.004165\n",
      " the is chise pat ester of inneind des ita the the sod trentigimal, fhimal suche the rserem the past orters reckersif reced to ealeicurdethen. SMerve stornt oreallins (1) is ere set ofse meral na0) it\n",
      "\n",
      "\n",
      "iter :10000, loss:10.884986\n",
      "eds the convutaur sequences it oumprecrentedworks are futpand sethe opttion, it of memeren temith, unneSion the next berective neral, (1) insuife therncy\n",
      "ly, are sedusrlork gfor cl pescien. To pagting\n",
      "\n",
      "\n",
      "iter :10500, loss:10.505638\n",
      "s, proputs neural networks, which ct seas wogsare pet ies are ofvie thien aral iet of ay is tiers.\n",
      "Af pperots recurrent in nebve sust of pake of of the copuran netwer, anget and seracs it op fogs whrm\n",
      "\n",
      "\n",
      "iter :11000, loss:9.351268\n",
      "s al-prebras lleutan. Toural gratien â€” in toreckser of ined ne) gikse ternerte tomernedimemich corpel mentogis geterring. which closely reserfon dathem aremsily, wonent X(2) from the raw input durre\n",
      "\n",
      "\n",
      "iter :11500, loss:8.281735\n",
      " RNNerlero vum rabl the next step and sol neus lfer of opthe. it meceine. A(1) from the ontering rredutif of paw input of hemen prod ha hy int reration. Itw reclrrengod an le turrenglome cas as prodie\n",
      "\n",
      "\n",
      "iter :12000, loss:7.697618\n",
      "stine the coung thiniza, if topercorst rand herognedice dale vatien. Fmeces the output of the cons atherd neucastion. Ather the sorvested. LSTM is wech somy frech-sem go (eceralll thesithe rakg X(1) i\n",
      "\n",
      "\n",
      "iter :12500, loss:7.037915\n",
      "sting Forecanf neural networks, which cakes thesolll vas lan in the outdet an wares the outidererverss the X(0) from the urathem...\n",
      "\n",
      "Af Arqeutwor therither muth therither from tie nepterssitheing rath\n",
      "\n",
      "\n",
      "iter :13000, loss:6.163936\n",
      " the ther timemoles atw ropaod la gsting nt andicingermed probles hras if anealfy ther inge calgnemvem the preveaz inthor. duman bedericus. co oucons consogtprmested. int. works andising then it easie\n",
      "\n",
      "\n",
      "iter :13500, loss:5.474220\n",
      "suraks al serreschise. LSTM is wally the caren the X(mothe t ore lawo sutre conv the inathyy networ sNier tomethe selied insta r Ne copnecs fram into wucks the output of pata thated. Antveer Simelitin\n",
      "\n",
      "\n",
      "iter :14000, loss:6.774076\n",
      "s ared processing.\n",
      "work. For making a lercos on to ine toritr. ited mot the input prstien and pro the output, proced cesor to it a(k input â€” inputwurpto itiog pratpemcksmog int or the suta whimerion\n",
      "\n",
      "\n",
      "iter :14500, loss:6.225769\n",
      "ter the next ce the input fo sequins nt can detwork. ited ther inte conney ast the cumory storilal, ,e tioiniza pere vesigntiritiz(won th instiop the outsernsuts sequencew ive from the sam the inputs \n",
      "\n",
      "\n",
      "iter :15000, loss:5.201301\n",
      "s -ousidam the pabd of geminr Spasilgo ate the humanding blectast recurre torether, prececognthe rath X(1) is the onemece putput, it haking in tarem outhing..FRNNce cimat anped hikent nelateproctl the\n",
      "\n",
      "\n",
      "iter :15500, loss:4.446356\n",
      "st-se taris menesognathech a(nas sse thear inte next step and so on. Thal seqsedicint he se from the sigat it oace desines aremationt of tichally aoghly racline perestort daral, h(1) from the conted, \n",
      "\n",
      "\n",
      "iter :16000, loss:4.736415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\simon\\Desktop\\VUT\\MIT1\\SFC\\project\\LSTM-demo\\rnn.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_reader \u001b[39m=\u001b[39m DataReader(\u001b[39m\"\u001b[39m\u001b[39minput.txt\u001b[39m\u001b[39m\"\u001b[39m, seq_length)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rnn \u001b[39m=\u001b[39m RNN(hidden_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, vocab_size\u001b[39m=\u001b[39mdata_reader\u001b[39m.\u001b[39mvocab_size,seq_length\u001b[39m=\u001b[39mseq_length,learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m rnn\u001b[39m.\u001b[39;49mtrain(data_reader)\n",
      "\u001b[1;32mc:\\Users\\simon\\Desktop\\VUT\\MIT1\\SFC\\project\\LSTM-demo\\rnn.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m data_reader\u001b[39m.\u001b[39mnext_batch()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39m# Forward pass \u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m xs, hs, ps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(inputs, hprev)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m dU, dW, dV, db, dc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(xs, hs, ps, targets)\n",
      "\u001b[1;32mc:\\Users\\simon\\Desktop\\VUT\\MIT1\\SFC\\project\\LSTM-demo\\rnn.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m xs[t] \u001b[39m=\u001b[39m zero_init(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m xs[t][inputs[t]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# one hot encoding , 1-of-k\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m hs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mU,xs[t]) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW,hs[t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb) \u001b[39m# hidden state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m os[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mV,hs[t]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc \u001b[39m# unnormalised log probs for next char\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m ycap[t] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(os[t]) \u001b[39m# probs for next char\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\simon\\Desktop\\VUT\\MIT1\\SFC\\project\\LSTM-demo\\rnn.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/simon/Desktop/VUT/MIT1/SFC/project/LSTM-demo/rnn.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnn\u001b[39m.\u001b[39mpredict(data_reader, \u001b[39m'\u001b[39m\u001b[39mget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "rnn.predict(data_reader, 'get', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
